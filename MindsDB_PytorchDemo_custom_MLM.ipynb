{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "minus-rhythm",
   "metadata": {},
   "source": [
    "## Pytorch Community Voices Demo - MindsDB\n",
    "### 2021.09.01\n",
    "\n",
    "Author: Natasha Seelam, PhD (natasha@mindsdb.com)\n",
    "\n",
    "In the following notebook, we'll show you how you can make your own custom encoder, and pipeline, using lightwood under the hood. The model we've chosen is a **Masked Language Model**, and specifically, we're going to treat it as if it were a classifier.\n",
    "\n",
    "**Please note, the typical classification strategy is to place a linear layer after the transformer and fine tune (this MindsDB's default). This example just demonstrates that we can still build our own custom encoder to leverage novel pytorch models with diverse loss functions and criteria.**\n",
    "\n",
    "Please feel free to email us, or join our community slack if you have any questions you'd like to discuss!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "compact-audit",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine learning\n",
    "import torch\n",
    "import lightwood as lw\n",
    "from lightwood import ProblemDefinition, JsonAI, json_ai_from_problem, code_from_json_ai, predictor_from_code\n",
    "\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "decent-black",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version 1.1.0\n"
     ]
    }
   ],
   "source": [
    "# Lightwood version > 1.0.0 supports J{ai}-SON : )\n",
    "print(\"Version\", lw.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "unauthorized-civilian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f1476ccf990>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup torch seed for reproducibility\n",
    "torch.manual_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-fitting",
   "metadata": {},
   "source": [
    "## 1) Load the dataset\n",
    "\n",
    "We'll use a tripadvisor hotel review dataset (found on [kaggle](https://www.kaggle.com/andrewmvd/trip-advisor-hotel-reviews)).\n",
    "The goal of this dataset is to be able to predict the rating (1-5 stars, with 1 being the lowest and 5 being the highest) of the hotel, given an arbitary review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "moral-assault",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mtripadvisor\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "%ls datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "handy-maldives",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10726</th>\n",
       "      <td>not recommend hotel did reviewers actually sta...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14919</th>\n",
       "      <td>barcelona rocks, stayed hotel jazz girlfriend ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19098</th>\n",
       "      <td>ok hotel good location stayed night way beijin...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2450</th>\n",
       "      <td>great service nice pool ok beach lovely ground...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>surprising treat spent weekend july 15/16 2006...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Review  Rating\n",
       "10726  not recommend hotel did reviewers actually sta...       1\n",
       "14919  barcelona rocks, stayed hotel jazz girlfriend ...       4\n",
       "19098  ok hotel good location stayed night way beijin...       3\n",
       "2450   great service nice pool ok beach lovely ground...       4\n",
       "960    surprising treat spent weekend july 15/16 2006...       5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"datasets/\"\n",
    "dataset = \"tripadvisor\"\n",
    "filename = os.path.join(data_dir, dataset, \"data.csv\")\n",
    "\n",
    "\n",
    "# Load the data, and scramble the order. The way the data has been \n",
    "data = pd.read_csv(filename).sample(frac=1, random_state=42)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-michael",
   "metadata": {},
   "source": [
    "We can see the polarity between a 5-star and 1-star rating. Positive words are more associated to good reviews versus one-star labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "active-season",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mFive star (Label=5) Review:\n",
      "\u001b[0m\n",
      "surprising treat spent weekend july 15/16 2006 cartwright hotel based purely recommendations read site, actually expecting like small older european hotel, cartwright amazing, small elegant pleasant staff knowlegable city, room small nicely appointed queen bed linens quality, no airconditioning temperatures sf rarely warrant open windows fresh air night 4th floor, wine hour afternoon gave opportunity meet guests share days adventures breakfast buffet adequate, walk half block powell st cable car usually crowded stop, sf taxi cabs clean reasonably priced usually came built tour guide unlike cities english optional business, cartwright definitely hotel choice return san francisco,  \n",
      "\u001b[1m\n",
      "\n",
      "One star (Label=1) Review: \n",
      "\u001b[0m\n",
      "not recommend hotel did reviewers actually stay hotel did, good thing hotel location really close leidseplein, shared facilities filthy got, did not look toilet floor cleaned month, facilities not cleaned 3 days got, disgusting, staff rude complained left night early refused refund night, not recommend hotel,  \n"
     ]
    }
   ],
   "source": [
    "# Rating examples\n",
    "print(\"\\033[1m\" + \"Five star (Label=5) Review:\\n\" + \"\\033[0m\")\n",
    "print(data[data[\"Rating\"] == 5][\"Review\"].iloc[0])\n",
    "\n",
    "print(\"\\033[1m\" + \"\\n\\nOne star (Label=1) Review: \\n\" + \"\\033[0m\")\n",
    "print(data[data[\"Rating\"] == 1][\"Review\"].iloc[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-baseline",
   "metadata": {},
   "source": [
    "The ratings distribution is not equivalent across all labels; while we will use accuracy, we encourage considering other metrics to predict on such as precision, recall, and accuracy. For simplicity, we merely consider the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "external-springfield",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Ratings distribution')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# An example of the distribution of the model \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Distribution of the ratings\n",
    "fig, ax = plt.subplots(1,1, figsize=(6,4))\n",
    "\n",
    "hist, bins = np.histogram(data[\"Rating\"], bins=np.arange(0.5, 6.5, 1))\n",
    "\n",
    "ax.bar(bins[:-1], hist.astype(np.float32) / hist.sum(), width=0.5)\n",
    "ax.set_xticks([i+0.5 for i in range(5)])\n",
    "ax.set_xticklabels([str(i+1) for i in range(5)])\n",
    "ax.set_yticks(np.arange(0, 0.55, 0.05))\n",
    "\n",
    "ax.set_xlabel(\"Ratings\")\n",
    "ax.set_ylabel(\"Percent of Dataset\")\n",
    "ax.set_title(\"Ratings distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-romance",
   "metadata": {},
   "source": [
    "As indicated before, we will split our data into a training/testing branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "featured-genius",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll split our training/testing fraction manually\n",
    "\n",
    "ptrain = 0.8\n",
    "Ntrain = int(data.shape[0] * ptrain)\n",
    "Ntest = int(data.shape[0] - Ntrain)\n",
    "\n",
    "train = data.iloc[:Ntrain, :]\n",
    "test = data.iloc[Ntrain:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "actual-burner",
   "metadata": {},
   "source": [
    "## 2) Building your own custom encoder\n",
    "\n",
    "**NOTE**: *When I tested this encoder, I found I needed at least 8GB of VRAM and a GPU.*\n",
    "\n",
    "For our custom encoder, we will use a **masked language model** (MLM). To explain how an MLM works, imagine a sentence like this:\n",
    "\n",
    "\"This hotel is amazing! I loved my stay here, the bed was comfy and everything was perfect!\"\n",
    "\n",
    "We can convert this into a classification problem using MLM as such:\n",
    "\n",
    "\"Label is [MASK]. This hotel is amazing! I loved my stay here, the bed was comfy and everything was perfect!\"\n",
    "\n",
    "The goal would be to predict whether the hidden token is a positive or negative sentiment. To do this, we will construct \"placeholder\" tokens that represent the hidden label. The model then computes losses to try to predict what hidden token should replace the underlying label. The ground-truth label will be one of the placeholders we setup.\n",
    "\n",
    "For convenience, we have provided 2 different scripts, `custom_encoder.py` and `mlm_helpers.py`. The `custom_encoder` script provides an example of how to build an MLM for classification, and `mlm_helpers`, as the name suggests, provides a few functions to help construct labels, new vocabulary, and train a model. \n",
    "\n",
    "We will show how to integrate this as a separate branch in the lightwood repo, and train with your own encoder of choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valued-thailand",
   "metadata": {},
   "source": [
    "### 2a) Making the custom encoder files \n",
    "\n",
    "The goal of the encoder is to learn a representation. Some encoders can be trained or \"tuned\" to generate a representation related to a particular label to ensure the representations contain useful information. In this simple example, we will tune the encoder with the target data.\n",
    "\n",
    "In this case, a representation is a feature vector that translates the \"data\" space into a format that is usable by a model. The term \"feature vector\", \"representation\", and \"embedding\" will be used interchangeably hence forth.\n",
    "\n",
    "![This is the caption\\label{mylabel}](images/fig1.png)\n",
    "*Figure 1: A representation will take an arbitrary sentence, like this hotel review, and convert it into some vector that a model can use. How the vector is constructed depends on the modeling of the language.*\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "located-facial",
   "metadata": {},
   "source": [
    "In the following example in ```custom_encoder.py``` we consider the ```MLMEncoder``` class. All base encoders require the following steps:\n",
    "\n",
    "(1) an ```__init__``` call used to instantitate the details of the encoder. The only required component of this call is to ensure that the encoder knows whether you are tuning the embeddings to the target or not.  \n",
    "\n",
    "(2) A ```prepare``` call; This sets up the preliminary ground work of tokenizing and training the encoder to the representation of interest. You can think of this as the ***feature extraction step***\n",
    "\n",
    "(3) An ```encode``` command; this creates a representation/embedding that will be provided downstream to the mixer or predictive model of the piepline .\n",
    "\n",
    "(4) A ```decode``` step; This steps translated the featurized form back into the original space. For text, we implemented an Exception, as decoding will be tricky (as an aside -one possible option is to decode back into the tokenized space for language generation!)\n",
    "\n",
    "(5) A ```to``` step; this step enables you to set the model to a different device (CPU/CUDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-mapping",
   "metadata": {},
   "source": [
    "For specific highlights of the custom encoder, see the script available. As a high level overview:\n",
    "\n",
    "1) The prepare step calls the ```DistilBertForMaskedLanguageModeling``` model and ```DistilBertTokenizerFast``` for the model and tokenizer respectively  \n",
    "\n",
    "2) It prepares a priming sentence with a \"[MASK]\" to be predicted, with the help of ```mlm_helpers``` functions.  \n",
    "\n",
    "3) For each label, it creates a new token \"[Ci]\" where $i$ represents the label number and resizes it to the tokenizer and the model embedding dimensionality. The goal of the model is to predict which token must appear under the Mask.\n",
    "\n",
    "4) It trains the MLM to predict the mask from the underlying label for 1 epoch (default)  \n",
    "\n",
    "5) The encoder creates an embedding by predicting what the underlying mask token is, replacing it in the model input, and then calculating the last hidden-state of the base DistilBert model and presenting the \"[CLS]\" token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-hudson",
   "metadata": {},
   "source": [
    "For simplicity, we've made these encoders in a custom branch of lightwood. You can access it as:\n",
    "\n",
    "```git clone -b custom_mlm git@github.com:mindsdb/lightwood.git``` \n",
    "\n",
    "Alternatively, you can place the two python files in your own branch of lightwood and edit the ``__init__`` files accordingly, as we'll describe below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-pioneer",
   "metadata": {},
   "source": [
    "You can see within ```lightwood/encoder/text``` the new encoder ```custom_encoders.py``` script.\n",
    "\n",
    "Subsequently, for the helpers file, it is in ```lightwood/encoders/text/helpers```.\n",
    "\n",
    "Now, we need to provide references for lightwood to access these encoders. Change the ```__init__.py``` file in ```lightwood/encoder/text/``` and ```lightwood/encoder/``` to enable lightwood to \"see\" your encoders.\n",
    "\n",
    "![This is the caption\\label{mylabel}](images/fig2.png)\n",
    "<br>\n",
    "\n",
    "\n",
    "Lastly, make sure to add the `unit_classifier` script to `lightwood/model/` and change the `__init__.py` file there too.\n",
    "\n",
    "In the spirit of lightwood, we have the following:<br>\n",
    "(1) Data pre-processing <br>\n",
    "(2) Feature engineering *via* Encoders <br>\n",
    "(3) Model building *via* Models<br>.\n",
    "\n",
    "Under the hood, we'll see that lightwood will recognize the types of each data column (Review - rich text, Rating - categorical). What we want to customize is the feature engineering and model building.\n",
    "\n",
    "Typically, the feature engineering for text would be to create this fixed length embedding. If we had other columns of data, we could concatenate this vector with the other data outputs. However, we have only one output, so we'll do the following:\n",
    "\n",
    "(A) Train an encoder to predict the masked token. This will be the transformer (neural network) that is trained\n",
    "(B) Use a model that inherits the encoder, takes the logits of the output and soft-maxes it. We call this the `UnitClassifier` model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-oklahoma",
   "metadata": {},
   "source": [
    "## 3) Testing your new encoder out!\n",
    "\n",
    "Let's now go ahead and use the J{ai}SON language to run our custom encoder.\n",
    "\n",
    "First, we need to define the problem-definition. All we need to specify is the goal of what we're trying to predict. In this case, the output will be `Rating`.\n",
    "\n",
    "Once we specify a problem definition, we can feed into J{ai}SON using `json_ai_from_problem`. We need to specify our input dataframe (`data`) and the problem definition before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "diagnostic-browse",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightwood-3276:Analyzing a sample of 10044\n",
      "INFO:lightwood-3276:from a total population of 20491, this is equivalent to 49.0% of your data.\n",
      "INFO:lightwood-3276:Using 15 processes to deduct types.\n",
      "INFO:lightwood-3276:Infering type for: Rating\n",
      "INFO:lightwood-3276:Infering type for: Review\n",
      "INFO:lightwood-3276:Column Rating has data type categorical\n",
      "INFO:lightwood-3276:Doing text detection for column: Review\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/natasha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "INFO:lightwood-3276:Starting statistical analysis\n",
      "INFO:lightwood-3276:Finished statistical analysis\n"
     ]
    }
   ],
   "source": [
    "# Create the problem definition\n",
    "\n",
    "problem_definition = {'target': 'Rating'}\n",
    "\n",
    "# Generate the j{ai}son syntax\n",
    "JsonAI = json_ai_from_problem(data, problem_definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "southern-registrar",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_filename = \"sample_customtext.json\"\n",
    "with open(json_filename, \"w\") as fp:\n",
    "   fp.write(JsonAI.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-eligibility",
   "metadata": {},
   "source": [
    "Under the hood, what we see is the following:\n",
    "\n",
    "(1) Lightwood takes a sample of your data to make some inferences on type (ex it sees 'Rating' as Categorical and 'Review' as text. <br>\n",
    "(2) It performs a statistical analysis around the distribution (ex: a histogram/bar chart of numerical quantities). <br>\n",
    "(3) Provides a JsonAI object that holds descriptors of the framework. <br>\n",
    "\n",
    "We want to modify the defaults so that we are using our own `MLMEncoder`. You may also want to feed in non-default keywords under the `\"args\": {}` component.\n",
    "\n",
    "Note, we will use the `UnitClassifier` model, a variant of our `Unit` model. Normally, for datasets with multiple columns, encoders featurize each column of data. We then can concatenate all those features for a vector that represents the multiple columns; lightwood then allows you to test multiple predictor algorithms to see which may work optimally (usually `Neural` for a neural network and `LightGBM` for the tree-based method. \n",
    "\n",
    "In this case, we only have 1 column of data, and 1 target. Training another model on top of the transformer is overkill, so in the interest of time, we omit all other models to explicitly inherit the encoder's model. This variant in particular clamps the output so it only returns a guess. In our internal modules, we keep an \"unknown\" class to help our models generalize to unseen distributions.\n",
    "\n",
    "Once the JSON input has been generated, we'll edit the feature's module, specifically for the 'Review' column as follows:\n",
    "![This is the caption\\label{mylabel}](images/fig3.png)\n",
    "<br>\n",
    "\n",
    "In general, you can manually modify any feature you want OR specify *a priori* if you're comfortable with the JSON format. This applies to features, or models, which are the predictors that actually generate the output. You can also edit models by specifying them under the tab above\n",
    "\n",
    "After you edit your JSON file (you can alternatively load \"sample_customtext_modified.json\"), go ahead and re-load it with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "statutory-australian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the json AI\n",
    "json_filename = \"sample_customtext_modified.json\"\n",
    "with open(json_filename, \"r\") as fp:\n",
    "   modified_json = JsonAI.from_json(fp.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cleared-compact",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the two new calls\n",
    "modified_json.imports = ['from lightwood.model import UnitClassifier', 'from lightwood.encoder import MLMEncoder']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-ending",
   "metadata": {},
   "source": [
    "Once you have this, you can convert the JsonAI into template python code to construct a predictor! This auto-generates all you need to run the framework end-to-end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "alleged-nelson",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "from lightwood.model import UnitClassifier\n",
      "from lightwood.encoder import MLMEncoder\n",
      "from lightwood.model import Neural\n",
      "from lightwood.model import LightGBM\n",
      "from lightwood.model import LightGBMArray\n",
      "from lightwood.model import SkTime\n",
      "from lightwood.model import Unit\n",
      "from lightwood.model import Regression\n",
      "from lightwood.ensemble import BestOf\n",
      "from lightwood.data import cleaner\n",
      "from lightwood.data import transform_timeseries, timeseries_analyzer\n",
      "from lightwood.data import splitter\n",
      "from lightwood.analysis import model_analyzer, explain\n",
      "from sklearn.metrics import r2_score, balanced_accuracy_score, accuracy_score\n",
      "import pandas as pd\n",
      "from lightwood.helpers.seed import seed\n",
      "from lightwood.helpers.log import log\n",
      "import lightwood\n",
      "from lightwood.api import *\n",
      "from lightwood.model import BaseModel\n",
      "from lightwood.encoder import BaseEncoder, __ts_encoders__\n",
      "from lightwood.encoder import Array, Binary, Categorical, Date, Datetime, Float, Image, Integer, Quantity, Rich_Text, Short_Text, Tags\n",
      "from lightwood.ensemble import BaseEnsemble\n",
      "from typing import Dict, List\n",
      "from lightwood.helpers.parallelism import mut_method_call\n",
      "from lightwood.data.encoded_ds import ConcatedEncodedDs\n",
      "from lightwood import ProblemDefinition\n",
      "from lightwood.api import PredictorInterface\n",
      "\n",
      "\n",
      "class Predictor(PredictorInterface):\n",
      "    target: str\n",
      "    models: List[BaseModel]\n",
      "    encoders: Dict[str, BaseEncoder]\n",
      "    ensemble: BaseEnsemble\n",
      "    mode: str\n",
      "\n",
      "    def __init__(self):\n",
      "        seed()\n",
      "        self.target = 'Rating'\n",
      "        self.mode = 'innactive'\n",
      "\n",
      "    def learn(self, data: pd.DataFrame) -> None:\n",
      "\n",
      "        # The type of each column\n",
      "        self.problem_definition = ProblemDefinition.from_dict({'target': 'Rating', 'nfolds': 30, 'pct_invalid': 1, 'unbias_target': False, 'seconds_per_model': 5263, 'seconds_per_encoder': 14134, 'time_aim': 8625.64385143053, 'target_weights': None, 'positive_domain': False, 'fixed_confidence': None, 'timeseries_settings': {'is_timeseries': False, 'order_by': None, 'window': None, 'group_by': None, 'use_previous_target': False, 'nr_predictions': None, 'historical_columns': None, 'target_type': ''}, 'anomaly_detection': True, 'anomaly_error_rate': None, 'anomaly_cooldown': True, 'ignore_features': [], 'fit_on_validation': True, 'strict_mode': True})\n",
      "        self.accuracy_functions = ['balanced_accuracy_score']\n",
      "        self.identifiers = {}\n",
      "        self.dtype_dict = {\n",
      "        'Rating': 'categorical',\n",
      "        'Review': 'mlmencoder'\n",
      "        }\n",
      "        self.statistical_analysis = lightwood.data.statistical_analysis(data, self.dtype_dict, {},\n",
      "                                                                        self.problem_definition)\n",
      "        self.mode = 'train'\n",
      "        # How columns are encoded\n",
      "        self.encoders = {\n",
      "        'Rating': Categorical.OneHotEncoder(is_target=True),\n",
      "        'Review': MLMEncoder(embed_mode=False,output_type=self.dtype_dict[self.target])\n",
      "        }\n",
      "        # Which column depends on which\n",
      "        self.dependencies = {\n",
      "        'Review': []\n",
      "        }\n",
      "        #\n",
      "        self.input_cols = ['Review']\n",
      "        \n",
      "        log.info('Cleaning the data')\n",
      "        data = cleaner(data=data,pct_invalid=self.problem_definition.pct_invalid,ignore_features=self.problem_definition.ignore_features,identifiers=self.identifiers,dtype_dict=self.dtype_dict,target=self.target,mode=self.mode,timeseries_settings=self.problem_definition.timeseries_settings,anomaly_detection=self.problem_definition.anomaly_detection)\n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "        nfolds = 30\n",
      "        log.info(f'Splitting the data into {nfolds} folds')\n",
      "        folds = splitter(data=data,k=nfolds,tss=self.problem_definition.timeseries_settings)\n",
      "        \n",
      "        log.info('Preparing the encoders')\n",
      "        \n",
      "        encoder_preping_dict = {}\n",
      "        enc_preping_data = pd.concat(folds[0:nfolds-1])\n",
      "        for col_name, encoder in self.encoders.items():\n",
      "            if not encoder.is_nn_encoder:\n",
      "                encoder_preping_dict[col_name] = [encoder, enc_preping_data[col_name], 'prepare']\n",
      "                log.info(f'Encoder preping dict length of: {len(encoder_preping_dict)}')\n",
      "        \n",
      "        parallel_preped_encoders = mut_method_call(encoder_preping_dict)\n",
      "        for col_name, encoder in parallel_preped_encoders.items():\n",
      "            self.encoders[col_name] = encoder\n",
      "        \n",
      "        if self.target not in parallel_preped_encoders:\n",
      "            self.encoders[self.target].prepare(enc_preping_data[self.target])\n",
      "        \n",
      "        for col_name, encoder in self.encoders.items():\n",
      "            if encoder.is_nn_encoder:\n",
      "                priming_data = pd.concat(folds[0:nfolds-1])\n",
      "                kwargs = {}\n",
      "                if self.dependencies[col_name]:\n",
      "                    kwargs['dependency_data'] = {}\n",
      "                    for col in self.dependencies[col_name]:\n",
      "                        kwargs['dependency_data'][col] = {\n",
      "                            'original_type': self.dtype_dict[col],\n",
      "                            'data': priming_data[col]\n",
      "                        }\n",
      "                    \n",
      "        \n",
      "                # This assumes target  encoders are also prepared in parallel, might not be true\n",
      "                if hasattr(encoder, 'uses_target'):\n",
      "                    kwargs['encoded_target_values'] = parallel_preped_encoders[self.target].encode(priming_data[self.target])\n",
      "        \n",
      "                encoder.prepare(priming_data[col_name], **kwargs)\n",
      "        \n",
      "            \n",
      "        \n",
      "\n",
      "        log.info('Featurizing the data')\n",
      "        encoded_ds_arr = lightwood.encode(self.encoders, folds, self.target)\n",
      "        train_data = encoded_ds_arr[0:int(nfolds*0.9)]\n",
      "        test_data = encoded_ds_arr[int(nfolds*0.9):]\n",
      "        \n",
      "        log.info('Training the models')\n",
      "        self.models = [UnitClassifier(target_encoder=self.encoders[self.target],stop_after=self.problem_definition.seconds_per_model)]\n",
      "        trained_models = []\n",
      "        for model in self.models:\n",
      "            try:\n",
      "                model.fit(train_data)\n",
      "                trained_models.append(model)\n",
      "            except Exception as e:\n",
      "                log.warning(f'Exception: {e} when training model: {model}')\n",
      "                if True and model.stable:\n",
      "                    raise e\n",
      "        \n",
      "        self.models = trained_models\n",
      "        \n",
      "        log.info('Ensembling the model')\n",
      "        self.ensemble = BestOf(data=test_data,accuracy_functions=self.accuracy_functions,target=self.target,models=self.models)\n",
      "        self.supports_proba = self.ensemble.supports_proba\n",
      "        \n",
      "        log.info('Analyzing the ensemble')\n",
      "        self.model_analysis, self.runtime_analyzer = model_analyzer(data=test_data,train_data=train_data,disable_column_importance=False,fixed_significance=None,confidence_normalizer=False,stats_info=self.statistical_analysis,ts_cfg=self.problem_definition.timeseries_settings,accuracy_functions=self.accuracy_functions,predictor=self.ensemble,target=self.target,dtype_dict=self.dtype_dict,positive_domain=self.statistical_analysis.positive_domain)\n",
      "        \n",
      "        # Partially fit the model on the reamining of the data, data is precious, we mustn't loss one bit\n",
      "        for model in self.models:\n",
      "            if True:\n",
      "                model.partial_fit(test_data, train_data)\n",
      "        \n",
      "\n",
      "    def predict(self, data: pd.DataFrame) -> pd.DataFrame:\n",
      "\n",
      "        self.mode = 'predict'\n",
      "        log.info('Cleaning the data')\n",
      "        data = cleaner(data=data,pct_invalid=self.problem_definition.pct_invalid,ignore_features=self.problem_definition.ignore_features,identifiers=self.identifiers,dtype_dict=self.dtype_dict,target=self.target,mode=self.mode,timeseries_settings=self.problem_definition.timeseries_settings,anomaly_detection=self.problem_definition.anomaly_detection)\n",
      "        \n",
      "        \n",
      "        \n",
      "        encoded_ds = lightwood.encode(self.encoders, data, self.target)[0]\n",
      "        encoded_data = encoded_ds.get_encoded_data(include_target=False)\n",
      "        \n",
      "\n",
      "        df = self.ensemble(encoded_ds)\n",
      "        insights = explain(data=data,encoded_data=encoded_data,predictions=df,ts_analysis=None,timeseries_settings=self.problem_definition.timeseries_settings,positive_domain=self.statistical_analysis.positive_domain,fixed_confidence=self.problem_definition.fixed_confidence,anomaly_detection=self.problem_definition.anomaly_detection,anomaly_error_rate=self.problem_definition.anomaly_error_rate,anomaly_cooldown=self.problem_definition.anomaly_cooldown,analysis=self.runtime_analyzer,target_name=self.target,target_dtype=self.dtype_dict[self.target])\n",
      "        return insights\n",
      "        \n",
      "\n",
      "\n",
      "    def predict_proba(self, data: pd.DataFrame) -> pd.DataFrame:\n",
      "\n",
      "        self.mode = 'predict'\n",
      "        log.info('Cleaning the data')\n",
      "        data = cleaner(data=data,pct_invalid=self.problem_definition.pct_invalid,ignore_features=self.problem_definition.ignore_features,identifiers=self.identifiers,dtype_dict=self.dtype_dict,target=self.target,mode=self.mode,timeseries_settings=self.problem_definition.timeseries_settings,anomaly_detection=self.problem_definition.anomaly_detection)\n",
      "        \n",
      "        \n",
      "        \n",
      "        encoded_ds = lightwood.encode(self.encoders, data, self.target)[0]\n",
      "        encoded_data = encoded_ds.get_encoded_data(include_target=False)\n",
      "        \n",
      "\n",
      "        df = self.ensemble(encoded_ds, predict_proba=True)\n",
      "        insights = explain(data=data,encoded_data=encoded_data,predictions=df,ts_analysis=None,timeseries_settings=self.problem_definition.timeseries_settings,positive_domain=self.statistical_analysis.positive_domain,fixed_confidence=self.problem_definition.fixed_confidence,anomaly_detection=self.problem_definition.anomaly_detection,anomaly_error_rate=self.problem_definition.anomaly_error_rate,anomaly_cooldown=self.problem_definition.anomaly_cooldown,analysis=self.runtime_analyzer,target_name=self.target,target_dtype=self.dtype_dict[self.target])\n",
      "        return insights\n",
      "        \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate some code\n",
    "code = code_from_json_ai(modified_json)\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-horizontal",
   "metadata": {},
   "source": [
    "From above, we see this is what a start-to-finish script would look like. We auto-generate all imports, necessary keywords, and details required to train your custom models. From here, all we need to do is make a predictor \"object\" and then train it!\n",
    "\n",
    "Let's make that predictor object and get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "patent-treaty",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightwood-3276:Starting statistical analysis\n",
      "INFO:lightwood-3276:Finished statistical analysis\n",
      "INFO:lightwood-3276:distilbert-base-uncased MLM text encoder\n",
      "INFO:lightwood-3276:Embedding mode off. Logits are output of encode()\n",
      "INFO:lightwood-3276:Cleaning the data\n",
      "INFO:lightwood-3276:Splitting the data into 30 folds\n",
      "INFO:lightwood-3276:Preparing the encoders\n",
      "INFO:lightwood-3276:Encoder preping dict length of: 1\n",
      "INFO:lightwood-3276:Done running for: Rating\n",
      "INFO:lightwood-3276:Training model.\n",
      "INFO:lightwood-3276:Preparing the training data\n",
      "INFO:lightwood-3276:Training the model\n",
      "/home/natasha/Documents/lightwood/lightwood/encoder/text/helpers/mlm_helpers.py:83: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "INFO:lightwood-3276:distilbert-base-uncased MLM text encoder at epoch 1 and loss 0.014342751270485569!\n",
      "INFO:lightwood-3276:MLM text encoder is prepared!\n",
      "INFO:lightwood-3276:Featurizing the data\n",
      "INFO:lightwood-3276:Training the models\n",
      "INFO:lightwood-3276:Unit Mixer just borrows from encoder\n",
      "INFO:lightwood-3276:Ensembling the model\n",
      "INFO:lightwood-3276:Model UnitClassifier obtained a best-of evaluation score of 0.1997\n",
      "INFO:lightwood-3276:Picked best model: UnitClassifier\n",
      "INFO:lightwood-3276:Analyzing the ensemble\n"
     ]
    }
   ],
   "source": [
    "# Turn the code above into a predictor object\n",
    "predictor = predictor_from_code(code)\n",
    "\n",
    "# Run the generated code on the dataset!\n",
    "predictor.learn(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-quarter",
   "metadata": {},
   "source": [
    "From the log file, you should see that we call the `distilbert-base-uncased MLM text encoder`. We also see that we deploy 2 encoders - one for Rating which represents it as a one-hot-encoded vector and one for the Review, which uses the MLM model. You'll see the term 'Training model' twice - this automatically flags for each encoder you deploy, whether it is rule based or trained.\n",
    "\n",
    "You'll see some notes from the MLM encoder - the output of the encode flag will be the logits of the class, or tokens we represented. The unit mixer will directly inherit that and make for an easy output.\n",
    "\n",
    "Let's now put this to the test - let's predict on our test set. The output will be a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "appropriate-bryan",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightwood-3276:Cleaning the data\n"
     ]
    }
   ],
   "source": [
    "result = predictor.predict(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-judge",
   "metadata": {},
   "source": [
    "We can see the output has the ground-truth values, the predictions from Lightwood, and the confidence based on the logits. Higher confidences suggest stronger conviction in the predictors. Given the model has a MASSIVE vocabulary, and the very shallow training we did, it's unsurprising our confidences are generally low:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "authentic-pickup",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>truth</th>\n",
       "      <th>prediction</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   truth prediction  confidence\n",
       "0      3          2        0.29\n",
       "1      1          2        0.19\n",
       "2      4          2        0.19\n",
       "3      5          5        0.39\n",
       "4      5          5        0.39"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observe a few instances\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-rehabilitation",
   "metadata": {},
   "source": [
    "Let's see how the distributions of ground-truth matched up with the predicted values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "sensitive-defense",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# An example of the distribution of the model \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Distribution of the ratings\n",
    "f, ax = plt.subplots(1, 2, figsize=(16,6))\n",
    "\n",
    "hist, bins = np.histogram(result[\"truth\"].astype(int), bins=np.arange(0.5, 6.5, 1))\n",
    "hist2, _ = np.histogram(result[\"prediction\"].astype(int), bins=np.arange(0.5, 6.5, 1))\n",
    "\n",
    "ax[0].bar(bins[:-1], hist.astype(np.float32) / hist.sum(), width=0.5)\n",
    "ax[0].set_xticks([i+0.5 for i in range(5)])\n",
    "ax[0].set_xticklabels([str(i+1) for i in range(5)])\n",
    "ax[0].set_yticks(np.arange(0, 0.55, 0.05))\n",
    "\n",
    "ax[0].set_xlabel(\"Ratings\")\n",
    "ax[0].set_ylabel(\"Percent of Dataset\")\n",
    "ax[0].set_title(\"Ground-truth Ratings\")\n",
    "\n",
    "\n",
    "ax[1].bar(bins[:-1], hist2.astype(np.float32) / hist2.sum(), width=0.5, color='r')\n",
    "ax[1].set_xticks([i+0.5 for i in range(5)])\n",
    "ax[1].set_xticklabels([str(i+1) for i in range(5)])\n",
    "ax[1].set_yticks(np.arange(0, 0.55, 0.05))\n",
    "\n",
    "ax[1].set_xlabel(\"Ratings\")\n",
    "ax[1].set_ylabel(\"Percent of Dataset\")\n",
    "ax[1].set_title(\"Prediction Ratings\")\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceramic-scratch",
   "metadata": {},
   "source": [
    "The model seems to allocate high reviews (4s and 5s) and low reviews (1s) together. It is possible that positive sentiment is clear in high reviews, but middling ones (like 2s and 3s) might be hard to differentiate, hence map to 2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mdb",
   "language": "python",
   "name": "mdb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
